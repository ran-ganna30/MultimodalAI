#🧠 Multimodal AI: Image-to-Caption Matching with CLIP

This project shows how to build a simple multimodal AI system using Python and OpenAI's CLIP model. The model compares an image with a list of possible captions and returns the ones that best describe the image.

🚀 What It Does
Loads and processes an input image

Embeds both image and text in the same semantic space using CLIP

Measures similarity between the image and each caption

Returns the top 5 captions that match the image best

🧰 Tech Stack
Python

PyTorch

Hugging Face Transformers (CLIP)

scikit-learn (for cosine similarity)

PIL (for image handling)



🔮 Use Cases
Smart image captioning

Product description generation

AI-powered image search

Multimodal chatbots

🙋‍♂️ Author
Ranveer Ganna
📫 rganna2@illinois.edu
💼 LinkedIn

